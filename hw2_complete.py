# -*- coding: utf-8 -*-
"""hw2_complete.py

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1IzEKyWaR6Q333Hu2gxlcWym2b6vzVySC
"""

import tensorflow as tf
from tensorflow import keras
from keras import layers, models
from keras.models import Sequential, Model
from keras.layers import DepthwiseConv2D, Conv2D, BatchNormalization, GlobalAveragePooling2D, Dense

def build_model1():
    model = models.Sequential()

    # Convolutional Layer 1
    model.add(layers.Conv2D(32, (3, 3), strides=(2, 2), padding='same', activation='relu', input_shape=(32, 32, 3)))
    model.add(layers.BatchNormalization())

    # Convolutional Layer 2
    model.add(layers.Conv2D(64, (3, 3), strides=(2, 2), padding='same', activation='relu'))
    model.add(layers.BatchNormalization())

    # Convolutional Layer 3
    model.add(layers.Conv2D(128, (3, 3), strides=(2, 2), padding='same', activation='relu'))
    model.add(layers.BatchNormalization())

    # Convolutional Layer 4
    model.add(layers.Conv2D(128, (3, 3), padding='same', activation='relu'))
    model.add(layers.BatchNormalization())

    # Convolutional Layer 5
    model.add(layers.Conv2D(128, (3, 3), padding='same', activation='relu'))
    model.add(layers.BatchNormalization())

    # Convolutional Layer 6
    model.add(layers.Conv2D(128, (3, 3), padding='same', activation='relu'))
    model.add(layers.BatchNormalization())

    # Convolutional Layer 7
    model.add(layers.Conv2D(128, (3, 3), padding='same', activation='relu'))
    model.add(layers.BatchNormalization())

    # MaxPooling Layer
    model.add(layers.MaxPooling2D((4, 4), strides=(4, 4)))

    # Flatten Layer
    model.add(layers.Flatten())

    # Dense Layer 1
    model.add(layers.Dense(128, activation='relu'))
    model.add(layers.BatchNormalization())

    # Dense Layer 2
    model.add(layers.Dense(10, activation='softmax'))

    return model

def build_model2():
    model = models.Sequential()

    # Convolutional Layer 1
    model.add(layers.Conv2D(32, (3, 3), strides=(2, 2), padding='same', activation='relu', input_shape=(32, 32, 3)))
    model.add(layers.BatchNormalization())

    # Depthwise Separable Convolutional Layer 2
    model.add(layers.SeparableConv2D(64, (3, 3), strides=(2, 2), padding='same', activation='relu'))
    model.add(layers.BatchNormalization())

    # Depthwise Separable Convolutional Layer 3
    model.add(layers.SeparableConv2D(128, (3, 3), strides=(2, 2), padding='same', activation='relu'))
    model.add(layers.BatchNormalization())

    # Depthwise Separable Convolutional Layer 4
    model.add(layers.SeparableConv2D(128, (3, 3), padding='same', activation='relu'))
    model.add(layers.BatchNormalization())

    # Depthwise Separable Convolutional Layer 5
    model.add(layers.SeparableConv2D(128, (3, 3), padding='same', activation='relu'))
    model.add(layers.BatchNormalization())

    # Depthwise Separable Convolutional Layer 6
    model.add(layers.SeparableConv2D(128, (3, 3), padding='same', activation='relu'))
    model.add(layers.BatchNormalization())

    # Depthwise Separable Convolutional Layer 7
    model.add(layers.SeparableConv2D(128, (3, 3), padding='same', activation='relu'))
    model.add(layers.BatchNormalization())

    # MaxPooling Layer
    model.add(layers.MaxPooling2D((4, 4), strides=(4, 4)))

    # Flatten Layer
    model.add(layers.Flatten())

    # Dense Layer 1
    model.add(layers.Dense(128, activation='relu'))
    model.add(layers.BatchNormalization())

    # Dense Layer 2
    model.add(layers.Dense(10, activation='softmax'))

    return model

def build_model3():
    inputs = Input(shape=(32, 32, 3))

    # Convolutional Layer 1
    x = layers.Conv2D(32, (3, 3), strides=(2, 2), padding='same', activation='relu')(inputs)
    x = layers.BatchNormalization()(x)

    # Convolutional Layer 2
    x = layers.Conv2D(64, (3, 3), strides=(2, 2), padding='same', activation='relu')(x)
    x = layers.BatchNormalization()(x)
    x = Dropout(0.5)(x)

    # Convolutional Layer 3 with residual connection
    x_shortcut = x
    x = layers.Conv2D(128, (1, 1), strides=(2, 2), padding='same', activation='relu')(x)
    x = layers.BatchNormalization()(x)
    x = Dropout(0.5)(x)
    x_shortcut = layers.Conv2D(128, (1, 1), strides=(2, 2), padding='same')(x_shortcut)
    x = Add()([x, x_shortcut])

    # Convolutional Layer 4
    x = layers.Conv2D(128, (3, 3), padding='same', activation='relu')(x)
    x = layers.BatchNormalization()(x)
    x = Dropout(0.5)(x)

    # Convolutional Layer 5 with residual connection
    x_shortcut = x
    x = layers.Conv2D(128, (3, 3), padding='same', activation='relu')(x)
    x = layers.BatchNormalization()(x)
    x = Dropout(0.5)(x)
    x_shortcut = layers.Conv2D(128, (1, 1), padding='same')(x_shortcut)
    x = Add()([x, x_shortcut])

    # Convolutional Layer 6
    x = layers.Conv2D(128, (3, 3), padding='same', activation='relu')(x)
    x = layers.BatchNormalization()(x)
    x = Dropout(0.5)(x)

    # Convolutional Layer 7 with residual connection
    x_shortcut = x
    x = layers.Conv2D(128, (3, 3), padding='same', activation='relu')(x)
    x = layers.BatchNormalization()(x)
    x = Dropout(0.5)(x)
    x_shortcut = layers.Conv2D(128, (1, 1), padding='same')(x_shortcut)
    x = Add()([x, x_shortcut])

    # MaxPooling Layer
    x = layers.MaxPooling2D((4, 4), strides=(4, 4))(x)

    # Flatten Layer
    x = layers.Flatten()(x)

    # Dense Layer 1
    x = layers.Dense(128, activation='relu')(x)
    x = layers.BatchNormalization()(x)

    # Dense Layer 2
    outputs = layers.Dense(10, activation='softmax')(x)

    model = models.Model(inputs, outputs)

    return model

if __name__ == '__main__':
    # Load CIFAR-10 dataset and split into train, validation, and test sets
    (train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.cifar10.load_data()
    train_images = train_images.astype('float32') / 255.0
    test_images = test_images.astype('float32') / 255.0
    val_split = 0.1
    num_val_samples = int(val_split * len(train_images))
    val_images = train_images[:num_val_samples]
    val_labels = train_labels[:num_val_samples]
    train_images = train_images[num_val_samples:]
    train_labels = train_labels[num_val_samples:]

    ## Build and train model 1
    model1 = build_model1()
    model1.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
    model1.summary()
    history1 = model1.fit(train_images, train_labels, epochs=50, validation_data=(val_images, val_labels))

    ## Evaluate model 1 on test set
    test_loss1, test_acc1 = model1.evaluate(test_images, test_labels)
    print("Model 1 Test Accuracy:", test_acc1)

    ## Build, compile, and train model 2
    model2 = build_model2()
    model2.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
    model2.summary()
    history2 = model2.fit(train_images, train_labels, epochs=50, validation_data=(val_images, val_labels))

    ## Evaluate model 2 on test set
    test_loss2, test_acc2 = model2.evaluate(test_images, test_labels)
    print("Model 2 Test Accuracy:", test_acc2)

    ## Build, compile, and train model 3
    model3 = build_model3()
    model3.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
    model3.summary()
    history3 = model3.fit(train_images, train_labels, epochs=50, validation_data=(val_images, val_labels))

    ## Evaluate model 3 on test set
    test_loss3, test_acc3 = model3.evaluate(test_images, test_labels)
    print("Model 3 Test Accuracy:", test_acc3)

from PIL import Image
import numpy as np
from google.colab import drive
drive.mount('/content/drive')

# Load the image
image = Image.open('/content/drive/My Drive/Dodge.jpg')
# Resize the image to match the input size of the models (32x32 pixels)
image = image.resize((32, 32))
# Convert the image to a numpy array
image_array = np.array(image) / 255.0  # Normalize pixel values to [0, 1]

# Reshape the image array to match the input shape expected by the models (add batch dimension)
image_array = np.expand_dims(image_array, axis=0)

class_names = ["airplane", "automobile", "bird", "cat", "deer", "dog", "frog", "horse", "ship", "truck"]

# Classify the image using model1
predictions_model1 = model1.predict(image_array)
predicted_class_index_model1 = np.argmax(predictions_model1)
predicted_class_name_model1 = class_names[predicted_class_index_model1]
print("Predicted class using model 1:", predicted_class_name_model1)

# Classify the image using model2
predictions_model2 = model2.predict(image_array)
predicted_class_index_model2 = np.argmax(predictions_model2)
predicted_class_name_model2 = class_names[predicted_class_index_model2]
print("Predicted class using model 2:", predicted_class_name_model2)

# Classify the image using model3
predictions_model3 = model3.predict(image_array)
predicted_class_index_model3 = np.argmax(predictions_model3)
predicted_class_name_model3 = class_names[predicted_class_index_model3]
print("Predicted class using model 3:", predicted_class_name_model3)

import numpy as np
from keras import backend as K

def calculate_layer_stats(model):
    layer_stats = []

    for layer in model.layers:
        layer_type = layer.__class__.__name__
        output_shape = layer.output_shape
        num_parameters = layer.count_params()

        if 'Conv2D' in layer_type or 'Dense' in layer_type:
            input_shape = K.int_shape(layer.input)
            output_shape = K.int_shape(layer.output)
            if 'Conv2D' in layer_type:
                macs = np.prod(output_shape[1:]) * np.prod(layer.kernel_size) * input_shape[-1]
            elif 'Dense' in layer_type:
                macs = np.prod(input_shape[1:]) * np.prod(output_shape[1:])
        else:
            macs = 0

        layer_stats.append((layer_type, num_parameters, macs, output_shape))

    return layer_stats

# Calculate stats for each model
model1_stats = calculate_layer_stats(build_model1())
model2_stats = calculate_layer_stats(build_model2())
model3_stats = calculate_layer_stats(build_model3())

# Print stats for each model
for i, model_stats in enumerate([model1_stats, model2_stats, model3_stats], start=1):
    print(f"Model {i} stats:")
    print("Layer Type, # Parameters, # MACs, Output Shape")
    for layer_stat in model_stats:
        print(layer_stat)



import tensorflow as tf
from tensorflow import keras
from keras import layers, models
from keras.layers import Input, Conv2D, DepthwiseConv2D, BatchNormalization, GlobalAveragePooling2D, Dense, Dropout, Add
from keras.models import Model, Sequential
from PIL import Image
import numpy as np



def build_model1():
    model = models.Sequential()

    # Convolutional Layer 1
    model.add(layers.Conv2D(32, (3, 3), strides=(2, 2), padding='same', activation='relu', input_shape=(32, 32, 3)))
    model.add(layers.BatchNormalization())

    # Convolutional Layer 2
    model.add(layers.Conv2D(64, (3, 3), strides=(2, 2), padding='same', activation='relu'))
    model.add(layers.BatchNormalization())

    # Convolutional Layer 3
    model.add(layers.Conv2D(128, (3, 3), strides=(2, 2), padding='same', activation='relu'))
    model.add(layers.BatchNormalization())

    # Convolutional Layer 4
    model.add(layers.Conv2D(128, (3, 3), padding='same', activation='relu'))
    model.add(layers.BatchNormalization())

    # Convolutional Layer 5
    model.add(layers.Conv2D(128, (3, 3), padding='same', activation='relu'))
    model.add(layers.BatchNormalization())

    # Convolutional Layer 6
    model.add(layers.Conv2D(128, (3, 3), padding='same', activation='relu'))
    model.add(layers.BatchNormalization())

    # Convolutional Layer 7
    model.add(layers.Conv2D(128, (3, 3), padding='same', activation='relu'))
    model.add(layers.BatchNormalization())

    # MaxPooling Layer
    model.add(layers.MaxPooling2D((4, 4), strides=(4, 4)))

    # Flatten Layer
    model.add(layers.Flatten())

    # Dense Layer 1
    model.add(layers.Dense(128, activation='relu'))
    model.add(layers.BatchNormalization())

    # Dense Layer 2
    model.add(layers.Dense(10, activation='softmax'))

    return model

def build_model2():
    model = models.Sequential()

    # Convolutional Layer 1
    model.add(layers.Conv2D(32, (3, 3), strides=(2, 2), padding='same', activation='relu', input_shape=(32, 32, 3)))
    model.add(layers.BatchNormalization())

    # Depthwise Separable Convolutional Layer 2
    model.add(layers.SeparableConv2D(64, (3, 3), strides=(2, 2), padding='same', activation='relu'))
    model.add(layers.BatchNormalization())

    # Depthwise Separable Convolutional Layer 3
    model.add(layers.SeparableConv2D(128, (3, 3), strides=(2, 2), padding='same', activation='relu'))
    model.add(layers.BatchNormalization())

    # Depthwise Separable Convolutional Layer 4
    model.add(layers.SeparableConv2D(128, (3, 3), padding='same', activation='relu'))
    model.add(layers.BatchNormalization())

    # Depthwise Separable Convolutional Layer 5
    model.add(layers.SeparableConv2D(128, (3, 3), padding='same', activation='relu'))
    model.add(layers.BatchNormalization())

    # Depthwise Separable Convolutional Layer 6
    model.add(layers.SeparableConv2D(128, (3, 3), padding='same', activation='relu'))
    model.add(layers.BatchNormalization())

    # Depthwise Separable Convolutional Layer 7
    model.add(layers.SeparableConv2D(128, (3, 3), padding='same', activation='relu'))
    model.add(layers.BatchNormalization())

    # MaxPooling Layer
    model.add(layers.MaxPooling2D((4, 4), strides=(4, 4)))

    # Flatten Layer
    model.add(layers.Flatten())

    # Dense Layer 1
    model.add(layers.Dense(128, activation='relu'))
    model.add(layers.BatchNormalization())

    # Dense Layer 2
    model.add(layers.Dense(10, activation='softmax'))

    return model

def build_model3():
    inputs = Input(shape=(32, 32, 3))

    x = layers.Conv2D(32, kernel_size=(3,3), strides=(2,2), activation="relu", padding='same')(inputs)
    x = layers.BatchNormalization()(x)

    shortcut = x

    x = layers.Conv2D(64, kernel_size=(3,3), strides=(2,2), activation="relu", padding='same')(x)
    x = layers.BatchNormalization()(x)
    x = layers.Dropout(0.2)(x)

    x = layers.Conv2D(128, kernel_size=(3,3), strides=(2,2), activation="relu", padding='same')(x)
    x = layers.BatchNormalization()(x)

    shortcut = layers.Conv2D(128, kernel_size=(1, 1), strides=(4, 4), padding='same')(shortcut)
    x = layers.Add()([x, shortcut])
    shortcut2 = x

    for _ in range(2):
        x = layers.Conv2D(128, kernel_size=(3,3), activation="relu", padding='same')(x)
        x = layers.BatchNormalization()(x)
        x = layers.Dropout(0.2)(x)
    x = layers.Add()([x, shortcut2])
    shortcut3 = x

    for _ in range(2):
        x = layers.Conv2D(128, kernel_size=(3,3), activation="relu", padding='same')(x)
        x = layers.BatchNormalization()(x)
        x = layers.Dropout(0.2)(x)
    x = layers.Add()([x, shortcut3])

    x = layers.MaxPooling2D(pool_size=(4, 4))(x)
    x = layers.Flatten()(x)
    x = layers.Dense(128, activation='relu')(x)
    x = layers.BatchNormalization()(x)
    outputs = layers.Dense(10, activation='softmax')(x)

    # Create model
    model = tf.keras.Model(inputs=inputs, outputs=outputs)

    return model

if __name__ == '__main__':
    # Load CIFAR-10 dataset and split into train, validation, and test sets
    (train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.cifar10.load_data()
    train_images = train_images.astype('float32') / 255.0
    test_images = test_images.astype('float32') / 255.0
    val_split = 0.1
    num_val_samples = int(val_split * len(train_images))
    val_images = train_images[:num_val_samples]
    val_labels = train_labels[:num_val_samples]
    train_images = train_images[num_val_samples:]
    train_labels = train_labels[num_val_samples:]

    ## Build and train model 1
    model1 = build_model1()
    model1.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
    model1.summary()
    history1 = model1.fit(train_images, train_labels, epochs=50, validation_data=(val_images, val_labels))

    ## Evaluate model 1 on test set
    test_loss1, test_acc1 = model1.evaluate(test_images, test_labels)
    print("Model 1 Test Accuracy:", test_acc1)

    ## Build, compile, and train model 2
    model2 = build_model2()
    #model2.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
    #model2.summary()
    ##history2 = model2.fit(train_images, train_labels, epochs=50, validation_data=(val_images, val_labels))

    ## Evaluate model 2 on test set
    test_loss2, test_acc2 = model2.evaluate(test_images, test_labels)
    print("Model 2 Test Accuracy:", test_acc2)

    ## Build, compile, and train model 3
    model3 = build_model3()
    model3.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
    model3.summary()
    history3 = model3.fit(train_images, train_labels, epochs=50, validation_data=(val_images, val_labels))

    ## Evaluate model 3 on test set
    test_loss3, test_acc3 = model3.evaluate(test_images, test_labels)
    print("Model 3 Test Accuracy:", test_acc3)


def build_model50k():
    inputs = Input(shape=(32, 32, 3))

    # Convolutional Layer 1
    x = layers.Conv2D(32, (3, 3), strides=(2, 2), padding='same', activation='relu')(inputs)
    x = layers.BatchNormalization()(x)

    # Convolutional Layer 2
    x = layers.Conv2D(64, (3, 3), strides=(2, 2), padding='same', activation='relu')(x)
    x = layers.BatchNormalization()(x)
    x = Dropout(0.5)(x)

    # Convolutional Layer 3 with residual connection
    x_shortcut = x
    x = layers.Conv2D(128, (3, 3), strides=(2, 2), padding='same', activation='relu')(x)
    x = layers.BatchNormalization()(x)
    x = Dropout(0.5)(x)
    x_shortcut = layers.Conv2D(128, (1, 1), strides=(2, 2), padding='same')(x_shortcut)
    x = Add()([x, x_shortcut])

    # Convolutional Layer 4
    x = layers.Conv2D(128, (3, 3), padding='same', activation='relu')(x)
    x = layers.BatchNormalization()(x)
    x = Dropout(0.5)(x)

    # Convolutional Layer 5 with residual connection
    x_shortcut = x
    x = layers.Conv2D(128, (3, 3), padding='same', activation='relu')(x)
    x = layers.BatchNormalization()(x)
    x = Dropout(0.5)(x)
    x_shortcut = layers.Conv2D(128, (1, 1), padding='same')(x_shortcut)
    x = Add()([x, x_shortcut])

    # MaxPooling Layer
    x = layers.MaxPooling2D((4, 4), strides=(4, 4))(x)

    # Flatten Layer
    x = layers.Flatten()(x)

    # Dense Layer 1
    x = layers.Dense(128, activation='relu')(x)
    x = layers.BatchNormalization()(x)

    # Dense Layer 2
    outputs = layers.Dense(10, activation='softmax')(x)

    model = models.Model(inputs, outputs)

    return model

## Build, compile, train, save model50k
model50k = build_model50k()
model50k.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
history = model50k.fit(train_images, train_labels, epochs=50, validation_data=(val_images, val_labels))
test_loss, test_acc = model50k.evaluate(test_images, test_labels)
print("Model 50k Test Accuracy:", test_acc)
model50k.save("best_model.h5")

from PIL import Image
import numpy as np
from google.colab import drive
drive.mount('/content/drive')

# Load the image
image = Image.open('/content/drive/My Drive/Dodge.jpg')
# Resize the image to match the input size of the models (32x32 pixels)
image = image.resize((32, 32))
# Convert the image to a numpy array
image_array = np.array(image) / 255.0  # Normalize pixel values to [0, 1]

# Reshape the image array to match the input shape expected by the models (add batch dimension)
image_array = np.expand_dims(image_array, axis=0)

class_names = ["airplane", "automobile", "bird", "cat", "deer", "dog", "frog", "horse", "ship", "truck"]

# Classify the image using model1
predictions_model1 = model1.predict(image_array)
predicted_class_index_model1 = np.argmax(predictions_model1)
predicted_class_name_model1 = class_names[predicted_class_index_model1]
print("Predicted class using model 1:", predicted_class_name_model1)

# Classify the image using model2
predictions_model2 = model2.predict(image_array)
predicted_class_index_model2 = np.argmax(predictions_model2)
predicted_class_name_model2 = class_names[predicted_class_index_model2]
print("Predicted class using model 2:", predicted_class_name_model2)

# Classify the image using model3
predictions_model3 = model3.predict(image_array)
predicted_class_index_model3 = np.argmax(predictions_model3)
predicted_class_name_model3 = class_names[predicted_class_index_model3]
print("Predicted class using model 3:", predicted_class_name_model3)

import numpy as np
from keras import backend as K

def calculate_layer_stats(model):
    layer_stats = []

    for layer in model.layers:
        layer_type = layer.__class__.__name__
        output_shape = layer.output_shape
        num_parameters = layer.count_params()

        if 'Conv2D' in layer_type or 'Dense' in layer_type:
            input_shape = K.int_shape(layer.input)
            output_shape = K.int_shape(layer.output)
            if 'Conv2D' in layer_type:
                macs = np.prod(output_shape[1:]) * np.prod(layer.kernel_size) * input_shape[-1]
            elif 'Dense' in layer_type:
                macs = np.prod(input_shape[1:]) * np.prod(output_shape[1:])
        else:
            macs = 0

        layer_stats.append((layer_type, num_parameters, macs, output_shape))

    return layer_stats

# Calculate stats for each model
model1_stats = calculate_layer_stats(build_model1())
model2_stats = calculate_layer_stats(build_model2())
model3_stats = calculate_layer_stats(build_model3())
model50k_stats = calculate_layer_stats(build_model50k())

# Print stats for each model
for i, model_stats in enumerate([model1_stats, model2_stats, model3_stats], start=1):
    print(f"Model {i} stats:")
    print("Layer Type, # Parameters, # MACs, Output Shape")
    for layer_stat in model_stats:
        print(layer_stat)